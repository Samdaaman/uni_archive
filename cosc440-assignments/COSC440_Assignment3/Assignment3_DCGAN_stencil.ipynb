{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "Assignment3_DCGAN_stencil.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YxZslqOmwC9"
      },
      "source": [
        "# **Generative Adversarial Networks (GANs)**\n",
        "\n",
        "In this assignment, you will implement a Generative Adversarial Network which generates images of human faces. Your model will use a basic generator + discriminator setup, where training alternates between updating the generator and the discriminator. The generator will be an up-convolutional network which maps a random latent code to an image, and the discriminator will be a down-convolutional network which maps an image to a real/fake probability. The model you will implement will be based on the ‘DCGAN’ architecture described in this paper. DCGANs are a standard baseline for generative image-based modeling. They replace max pooling with convolutional stride, eliminate fully connected layers, and use transposed convolution for upsampling.\n",
        "\n",
        "### What is a GAN?\n",
        "\n",
        "In 2014, [Goodfellow et al.](https://arxiv.org/abs/1406.2661) presented a method for training generative models called Generative Adversarial Networks (GANs for short). In a GAN, we build two different neural networks. Our first network is a traditional classification network, called the **discriminator**. We will train the discriminator to take images, and classify them as being real (belonging to the training set) or fake (not present in the training set). Our other network, called the **generator**, will take random noise as input and transform it using a neural network to produce images. The goal of the generator is to fool the discriminator into thinking the images it produced are real.\n",
        "\n",
        "We can think of this back and forth process of the generator ($G$) trying to fool the discriminator ($D$), and the discriminator trying to correctly classify real vs. fake as a minimax game:\n",
        "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
        "where $x \\sim p_\\text{data}$ are samples from the input data ($\\sim$ just means 'drawn from' &mdash; so $x$ consists of our samples, each of which are 'drawn from' $p_\\text{data}$, our entire set of input data.), $z \\sim p(z)$ are the random noise samples, $G(z)$ are the generated images using the neural network generator $G$, and $D$ is the output of the discriminator, specifying the probability of an input being real. [Goodfellow et al.](https://arxiv.org/abs/1406.2661) analyze this minimax game and show how it relates to minimizing the Jensen-Shannon divergence between the training data distribution and the generated samples from $G$.\n",
        "\n",
        "To optimize this minimax game, we will alternate between taking gradient *descent* steps on the objective for $G$ (this corresponds to gradient ascent on the loss function), and gradient *ascent* steps on the objective for $D$:\n",
        "1. update the **generator** ($G$) to **minimize** the probability of the **discriminator** making the correct choice. \n",
        "2. update the **discriminator** ($D$) to **maximize** the probability of the **discriminator** making the correct choice.\n",
        "\n",
        "As a way to simplify the math, we **update the generator: maximize the probability of the discriminator making the incorrect choice**. This allows us to use two maximize functions as opposed to one minimize and one maximize. This is the standard update used in most GAN papers, and was used in the original paper from [Goodfellow et al.](https://arxiv.org/abs/1406.2661). \n",
        "\n",
        "In this assignment, we will alternate the following updates:\n",
        "1. Update the generator ($G$) to maximize the probability of the discriminator making the incorrect choice on generated data:\n",
        "$$\\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
        "2. Update the discriminator ($D$), to maximize the probability of the discriminator making the correct choice on real and generated data:\n",
        "$$\\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
        "\n",
        "### What else is there?\n",
        "Since 2014, GANs have exploded into a huge research area, with massive [workshops](https://sites.google.com/site/nips2016adversarial/), and [hundreds of new papers](https://paperswithcode.com/task/image-generation). Compared to other approaches for generative models, they often produce the highest quality samples but are some of the most difficult and finicky models to train (see [this github repo](https://github.com/soumith/ganhacks) that contains a set of 17 hacks that are useful for getting models working). Improving the stabiilty and robustness of GAN training is an open research question, with new papers coming out every day! For a more recent tutorial on GANs, see [here](https://arxiv.org/abs/1701.00160). There is also some even more recent exciting work that changes the objective function to Wasserstein distance and yields much more stable results across model architectures: [WGAN](https://arxiv.org/abs/1701.07875), [WGAN-GP](https://arxiv.org/abs/1704.00028)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D00emRL2mwC-"
      },
      "source": [
        "# **Setup**\n",
        "Here we define some useful visualization functions and download the data for testing your loss and activation functions. \n",
        "\n",
        "**If you are using this in colab, WHILE YOU ARE CODING, on the toolbar, click Runtime -> Change Runtime Type and make sure hardware accelerator is None** Because colab only allows a small amount of GPU time per day, you will want to debug your code first and then when you are confident it works, enable the GPU and train the full scale network for at least 2 epochs to see some quality images.\n",
        "\n",
        "NOTE: a full scale network takes about 1.5 hours to train for >5 epochs on a standard google colab backed GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0HaeH_BmwC_"
      },
      "source": [
        "!pip install tensorflow_gan\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "import imageio\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def display_images(images, save=None):\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "  for i in range(min(images.shape[0], 16)):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow((images[i, :, :, :] * 127.5 + 127.5).numpy().astype(np.uint8))\n",
        "      plt.axis('off')\n",
        "\n",
        "  if save is not None:\n",
        "      plt.savefig(save)\n",
        "  plt.show()\n",
        "\n",
        "## --------------------------------------------------------------------------------------\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_gan as tfgan\n",
        "\n",
        "# For evaluating the quality of generated images\n",
        "# Frechet Inception Distance measures how similar the generated images are to the real ones\n",
        "# https://nealjean.com/ml/frechet-inception-distance/\n",
        "# Lower is better\n",
        "module = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/inception_v3/classification/4\", output_shape=[1001])])\n",
        "def fid_function(real_image_batch, generated_image_batch):\n",
        "    \"\"\"\n",
        "    Given a batch of real images and a batch of generated images, this function pulls down a pre-trained inception \n",
        "    v3 network and then uses it to extract the activations for both the real and generated images. The distance of \n",
        "    these activations is then computed. The distance is a measure of how \"realistic\" the generated images are.\n",
        "\n",
        "    :param real_image_batch: a batch of real images from the dataset, shape=[batch_size, height, width, channels]\n",
        "    :param generated_image_batch: a batch of images generated by the generator network, shape=[batch_size, height, width, channels]\n",
        "\n",
        "    :return: the inception distance between the real and generated images, scalar\n",
        "    \"\"\"\n",
        "    INCEPTION_IMAGE_SIZE = (299, 299)\n",
        "    real_resized = tf.image.resize(real_image_batch, INCEPTION_IMAGE_SIZE)\n",
        "    fake_resized = tf.image.resize(generated_image_batch, INCEPTION_IMAGE_SIZE)\n",
        "    module.build([None, 299, 299, 3])\n",
        "    real_features = module(real_resized)\n",
        "    fake_features = module(fake_resized)\n",
        "    return tfgan.eval.frechet_classifier_distance_from_activations(real_features, fake_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Y8JULV7grd"
      },
      "source": [
        "##Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_fFMpSWkHdF"
      },
      "source": [
        "![ -f 'img_align_celeba.zip' ] || wget --content-disposition https://www.dropbox.com/s/o2cfewccmxqrg2x/img_align_celeba.zip?dl=0\n",
        "\n",
        "![ -d '/content/dataset' ] || unzip -q -d /content/dataset img_align_celeba.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pyFPIuOljzk"
      },
      "source": [
        "# Sets up tensorflow graph to load images\n",
        "# (This is the version using new-style tf.data API)\n",
        "def load_image_batch(dir_name, batch_size=128, shuffle_buffer_size=250000, n_threads=2):\n",
        "    \"\"\"\n",
        "    Given a directory and a batch size, the following method returns a dataset iterator that can be queried for \n",
        "    a batch of images\n",
        "\n",
        "    :param dir_name: a batch of images\n",
        "    :param batch_size: the batch size of images that will be trained on each time\n",
        "    :param shuffle_buffer_size: representing the number of elements from this dataset from which the new dataset will \n",
        "    sample\n",
        "    :param n_thread: the number of threads that will be used to fetch the data\n",
        "\n",
        "    :return: an iterator into the dataset\n",
        "    \"\"\"\n",
        "    # Function used to load and pre-process image files\n",
        "    # (Have to define this ahead of time b/c Python does allow multi-line\n",
        "    #    lambdas, *grumble*)\n",
        "    def load_and_process_image(file_path):\n",
        "        \"\"\"\n",
        "        Given a file path, this function opens and decodes the image stored in the file.\n",
        "\n",
        "        :param file_path: a batch of images\n",
        "\n",
        "        :return: an rgb image\n",
        "        \"\"\"\n",
        "        # Load image\n",
        "        image = tf.io.decode_jpeg(tf.io.read_file(file_path), channels=3)\n",
        "        image = tf.image.resize(image, [64,64])\n",
        "        \n",
        "        # Convert image to normalized float (0, 1)\n",
        "        image = tf.image.convert_image_dtype(image, tf.float32) / 256.0\n",
        "        # Rescale data to range (-1, 1)\n",
        "        image = (image - 0.5) * 2\n",
        "        return image\n",
        "\n",
        "    # List file names/file paths\n",
        "    dir_path = dir_name + '/*.jpg'\n",
        "    dataset = tf.data.Dataset.list_files(dir_path)\n",
        "\n",
        "    # Shuffle order\n",
        "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n",
        "\n",
        "    # Load and process images (in parallel)\n",
        "    dataset = dataset.map(map_func=load_and_process_image, num_parallel_calls=n_threads)\n",
        "\n",
        "    # Create batch, dropping the final one which has less than batch_size elements and finally set to reshuffle\n",
        "    # the dataset at the end of each iteration\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    # Prefetch the next batch while the GPU is training\n",
        "    dataset = dataset.prefetch(1)\n",
        "\n",
        "    # Return an iterator over this dataset\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAGblpxFJmeR"
      },
      "source": [
        "# **Building the Discriminator and the Generator**\n",
        "\n",
        "You should complete code here for your discriminator and generator models. You should build them using keras' Sequential model and layers and use the constant for GEN_DEPTH and KERNEL_DEPTH to define the depth (number of channels) of each layer. See writeup for more instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB3zpMasr00l"
      },
      "source": [
        "# GEN_DEPTH = 1024 #uncomment for the full scale model\n",
        "GEN_DEPTH = 128\n",
        "KERNEL_DEPTH = GEN_DEPTH // 2\n",
        "\n",
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(4*4*GEN_DEPTH, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.Reshape((4, 4, GEN_DEPTH)))\n",
        "    \n",
        "    #TODO: fill in the generator model layers\n",
        "    return model\n",
        "\n",
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    #TODO: fill in the discriminator model layers\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNQI6tK7Ntef"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOwVp7N0mwDN"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90VgitOliexu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2127463f-09c2-4140-9cee-1f164ac0c0ae"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images, batch_size):\n",
        "    noise = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return generated_images, gen_loss, disc_loss\n",
        "\n",
        "# Train the model for one epoch.\n",
        "def train_epoch(generator, discriminator, dataset_iterator, manager, batch_size, validate_image_seeds=None):\n",
        "    \"\"\"\n",
        "    Train the model for one epoch. Save a checkpoint every 500 or so batches.\n",
        "\n",
        "    :param generator: generator model\n",
        "    :param discriminator: discriminator model\n",
        "    :param dataset_ierator: iterator over dataset, see preprocess.py for more information\n",
        "    :param manager: the manager that handles saving checkpoints by calling save()\n",
        "\n",
        "    :return: The average FID score over the epoch\n",
        "    \"\"\"\n",
        "    # Loop over our data until we run out\n",
        "    for iteration, batch in enumerate(dataset_iterator):\n",
        "        generated_images, gen_loss, disc_loss = train_step(batch, batch_size)\n",
        "\n",
        "        # Calculate inception distance and track the fid in order\n",
        "        # to return the average\n",
        "        if iteration % 100 == 0:\n",
        "            print(\"iteration \",iteration, \" gen loss\", gen_loss.numpy(), \" disc loss\", disc_loss.numpy())\n",
        "            fid_ = fid_function(batch, generated_images)\n",
        "            print('**** INCEPTION DISTANCE: %g ****' % fid_)\n",
        "            if validate_image_seeds is not None:\n",
        "              display_images(generator(validate_image_seeds, training=False))\n",
        "            else:\n",
        "              display_images(generated_images)\n",
        "        if iteration % 500 == 0:\n",
        "            # Save\n",
        "            manager.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e628VEK1RAUh"
      },
      "source": [
        "#main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl9vwu0DSmjx"
      },
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "batch_size = 128\n",
        "\n",
        "img_dir = '/content/dataset/img_align_celeba'\n",
        "out_dir = './output'\n",
        "checkpoint_dir = './checkpoints'\n",
        "\n",
        "mode = 'train'\n",
        "# mode = 'test'\n",
        "restore_checkpoint = (mode == 'test') or False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXVAFwyA0YMO"
      },
      "source": [
        "# !rm -rf checkpoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTVuIu1wiucw"
      },
      "source": [
        "# Load a batch of images (to feed to the discriminator)\n",
        "dataset_iterator = load_image_batch(img_dir, batch_size=batch_size, n_threads=2)\n",
        "\n",
        "# Initialize generator and discriminator models\n",
        "generator = make_generator_model()\n",
        "discriminator = make_discriminator_model()\n",
        "\n",
        "#The discriminator and the generator optimizers are different since you will train two networks separately.\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
        "\n",
        "# For saving/loading models\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
        "# Ensure the output directory exists\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "if restore_checkpoint:\n",
        "    # restores the latest checkpoint using from the manager\n",
        "    checkpoint.restore(manager.latest_checkpoint) \n",
        "\n",
        "validate_image_seeds = tf.random.normal([batch_size, noise_dim])\n",
        "\n",
        "if mode == 'train':\n",
        "    for epoch in range(0, EPOCHS):\n",
        "        print('========================== EPOCH %d  ==========================' % epoch)\n",
        "        avg_fid = train_epoch(generator, discriminator, dataset_iterator, manager, \n",
        "                              batch_size=batch_size, validate_image_seeds=validate_image_seeds)\n",
        "        print(\"Average FID for Epoch: \" + str(avg_fid))\n",
        "        # Save at the end of the epoch, too\n",
        "        print(\"**** SAVING CHECKPOINT AT END OF EPOCH ****\")\n",
        "        manager.save()\n",
        "        display_images(generator(validate_image_seeds, training=False), \n",
        "                       save='image_at_epoch_{:04d}.png'.format(epoch))\n",
        "\n",
        "elif mode == 'test':\n",
        "    display_images(generator(validate_image_seeds, training=False), save='image_test.png')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkA81yOj21QD"
      },
      "source": [
        "Assignment used with permission and significantly modified for use in COSC440 from CS1470 TA Staff | Computer Science Department | Brown University"
      ]
    }
  ]
}